{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"inference.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGtQZd9h_-uf","executionInfo":{"status":"ok","timestamp":1609514799706,"user_tz":-330,"elapsed":24483,"user":{"displayName":"Faiz Malik","photoUrl":"","userId":"15357754248939835417"}},"outputId":"f9083966-880b-4cda-a6a8-69aa554fbd34"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PBdlS04ZOdYY"},"source":["# inference on video"]},{"cell_type":"code","metadata":{"id":"wTNsADZMOmqA"},"source":["! pip install Pillow-SIMD\r\n","! pip install ffmpeg-python\r\n","! pip install facenet-pytorch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3j9gPQcEOwJq"},"source":["import sys\r\n","sys.path.insert(0,'/content/drive/My Drive/my_projects/ISLR/Real-time-GesRec-master')\r\n","sys.path.insert(1,'/content/drive/My Drive/my_projects/ISLR')\r\n","sys.path.insert(2,'/content/drive/My Drive/my_projects/ISLR/app')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6x1FlIRO2jK"},"source":["import torch\r\n","from torch.autograd import Variable\r\n","import cv2\r\n","from PIL import Image\r\n","from spatial_transforms_new import *\r\n","from utils import Queue, LevenshteinDistance\r\n","import time\r\n","import torch.nn.functional as F\r\n","\r\n","from load_model import Opt, load_models\r\n","\r\n","from IPython.display import display,Audio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0XcTRdCO5vS"},"source":["import ffmpeg\r\n","def check_rotation(path_video_file):\r\n","    # this returns meta-data of the video file in form of a dictionary\r\n","    meta_dict = ffmpeg.probe(path_video_file)\r\n","\r\n","    # from the dictionary, meta_dict['streams'][0]['tags']['rotate'] is the key\r\n","    # we are looking for\r\n","    rotateCode = None\r\n","  #  print(meta_dict['streams'][0]['tags'].keys())\r\n","    if 'rotate' in meta_dict['streams'][0]['tags'].keys():      \r\n","        if int(meta_dict['streams'][0]['tags']['rotate']) == 90:\r\n","            rotateCode = cv2.ROTATE_90_CLOCKWISE\r\n","        elif int(meta_dict['streams'][0]['tags']['rotate']) == 180:\r\n","            rotateCode = cv2.ROTATE_180\r\n","        elif int(meta_dict['streams'][0]['tags']['rotate']) == 270:\r\n","            rotateCode = cv2.ROTATE_90_COUNTERCLOCKWISE\r\n","\r\n","    return rotateCode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_ihupL2O-KA"},"source":["class Prediction():\r\n","  def __init__(self,rotate_code=0):\r\n","    self.root_path='/content/drive/My Drive/my_projects/ISLR'\r\n","    self.audio_files_path='/content/drive/MyDrive/my_projects/ISLR/audio_files/'\r\n","    self.no_cuda=False\r\n","    self.top2_diff_threshold=.75\r\n","    self.freq_threshold=3\r\n","    self._6_freq_threshold=3#6\r\n","    self.pred_cls_queue_size=12\r\n","\r\n","    self.n_classes_clf=11\r\n","    self.n_classes_det=2\r\n","    self.det_queue_size=4\r\n","    self.clf_queue_size=4\r\n","    self.sample_duration=16\r\n","    self.sample_duration_det=8\r\n","    self.det_strategy='median'\r\n","    self.clf_strategy='ma'\r\n","\r\n","\r\n","    self.mean=[114.7748, 107.7354, 99.475]\r\n","\r\n","   # self.interpolation=Image.BICUBIC\r\n","    self.interpolation=Image.BILINEAR\r\n","\r\n","\r\n","\r\n","    self.pred_cls_queue=(np.ones(self.pred_cls_queue_size)*10).tolist()\r\n","\r\n","    self.clf_selected_queue = np.zeros(self.n_classes_clf, )\r\n","    self.det_selected_queue = np.zeros(self.n_classes_det, )\r\n","    self.myqueue_det = Queue(self.det_queue_size, n_classes=self.n_classes_det)\r\n","    self.myqueue_clf = Queue(self.clf_queue_size, n_classes=self.n_classes_clf)\r\n","\r\n","    self.num_frame = 0\r\n","    self.clip = []\r\n","    self.org_frms=[]\r\n","    opt =Opt(root_path=self.root_path,path_det='pretrained_models/egogesture_resnetl_10_RGB_8.pth',path_clf='my_model/n/with_none/merged_testset/best_acc_model_state_dict.pth',no_cuda=False)\r\n","    self.detector,self.classifier=load_models(opt)\r\n","    #self.detector=torch.load(self.root_path+'/my_model/detector_whole_model.pth')\r\n","    #self.classifier=torch.load(self.root_path+'/my_model/best_classifier_whole_model.pth')\r\n","    #self.classifier=classifier       \r\n","    if not self.no_cuda:\r\n","      self.detector,self.classifier=self.detector.cuda(),self.classifier.cuda()\r\n","\r\n","    self.detector.eval()\r\n","    self.classifier.eval()\r\n","        \r\n","    self.infer_extract_roi=InferenceExtractRoi(self.no_cuda)\r\n","\r\n","    self.spatial_transform = Compose([\r\n","        FinalSize(interpolation=self.interpolation),\r\n","        ToTensor(1), Normalize(self.mean, [1, 1, 1])\r\n","    ])\r\n","\r\n","    self.spatial_transform.randomize_parameters()\r\n","\r\n","    self.rotate_code=rotate_code\r\n","\r\n","    self.load_audio_files(self.audio_files_path)\r\n","    self.labels_list=[0,1,2,3,4,5,6,7,8,9]\r\n","    self.predictions_list=[]\r\n","    self.final_prediction=10\r\n","    self.prev_prediction=10\r\n","    self.count_0=0\r\n","    \r\n","    self.tmp_inputs2=0\r\n","    self.prev_best1=10\r\n","    self.top2_diff_avg=[]\r\n","    self.pred_cls=[]\r\n","    self.outputs_clf=0             #### temp\r\n","\r\n","  def  __call__(self,frame):\r\n","      t1 = time.time()\r\n","      if self.rotate_code is not None:\r\n","        frame=cv2.rotate(frame,self.rotate_code)\r\n","      if self.num_frame == 0:\r\n","          cur_frame=frame#cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)      ####\r\n","          for i in range(16):\r\n","              self.org_frms.append(cur_frame)\r\n","      self.org_frms.pop(0)\r\n","      _frame=frame#cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)             ####\r\n","      self.org_frms.append(_frame)\r\n","      x1,y1,x2,y2=self.infer_extract_roi(self.org_frms[0])\r\n","    #  st=time.time()\r\n","      self.clip = [self.spatial_transform(img[y1:y2,x1:x2].copy()) for img in self.org_frms]\r\n","      # print('time taken by transformation',time.time()-st)\r\n","\r\n","      im_dim = self.clip[0].size()[-2:]\r\n","      try:\r\n","          test_data = torch.cat(self.clip, 0).view((self.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\r\n","      except Exception as e:\r\n","          pdb.set_trace()\r\n","          raise e\r\n","      inputs = torch.cat([test_data],0).view(1,3,self.sample_duration,112,112)\r\n","      self.num_frame += 1\r\n","\r\n","\r\n","\r\n","      with torch.no_grad():\r\n","          inputs = Variable(inputs)\r\n","          inputs_det = inputs[:, [2,1,0], -self.sample_duration_det:, :, :]#inputs[:,:, -self.sample_duration_det:, :, :]\r\n","          outputs_det = self.detector(inputs_det)\r\n","          outputs_det = F.softmax(outputs_det, dim=1)\r\n","          outputs_det = outputs_det.cpu().numpy()[0].reshape(-1, )\r\n","          # enqueue the probabilities to the detector queue\r\n","          self.myqueue_det.enqueue(outputs_det.tolist())\r\n","\r\n","          self.myqueue_det.dequeue()        ####\r\n","\r\n","          if self.det_strategy == 'raw':\r\n","              self.det_selected_queue = outputs_det\r\n","          elif self.det_strategy == 'median':\r\n","              self.det_selected_queue = self.myqueue_det.median\r\n","          elif self.det_strategy == 'ma':\r\n","              self.det_selected_queue = self.myqueue_det.ma\r\n","          elif self.det_strategy == 'ewma':\r\n","              self.det_selected_queue = self.myqueue_det.ewma\r\n","          prediction_det = np.argmax(self.det_selected_queue)\r\n","\r\n","          prob_det = self.det_selected_queue[prediction_det]\r\n","\r\n","          #print(self.tmp_count,'\\n',outputs_det,'\\t',outputs_det.argmax())\r\n","         # print(self.tmp_count,'\\n',self.det_selected_queue,'\\t',prediction_det)\r\n","\r\n","          if prediction_det==0:\r\n","             self.count_0+=1\r\n","          else:\r\n","             self.count_0=0\r\n","\r\n","          if self.count_0>=50:\r\n","             prediction_det=0\r\n","          else:\r\n","             prediction_det=1\r\n","          \r\n","         # print('final_prediction_det:',prediction_det)\r\n","          \r\n","\r\n","          #### State of the detector is checked here as detector act as a switch for the classifier\r\n","          if prediction_det==1:                                                                      ####\r\n","              inputs_clf = inputs[:, :, :, :, :]\r\n","              inputs_clf = torch.Tensor(inputs_clf.numpy()[:,:,::1,:,:])\r\n","            #  st=time.time()\r\n","              outputs_clf = self.classifier(inputs_clf)\r\n","            #  print('actual time by clf',time.time()-st)\r\n","              #self.outputs_clf=outputs_clf\r\n","\r\n","              outputs_clf = F.softmax(outputs_clf.data, dim=1)\r\n","            #   s=time.time()\r\n","              outputs_clf=outputs_clf.detach().cpu()\r\n","          #    print('on cpu',time.time()-s)\r\n","              outputs_clf = outputs_clf.numpy()[0].reshape(-1, )\r\n","            #   print('cpu conversion',time.time()-s)\r\n","\r\n","              # Push the probabilities to queue\r\n","              #print(outputs_clf.shape,outpuu)\r\n","              self.myqueue_clf.enqueue(outputs_clf.tolist())\r\n","\r\n","              self.myqueue_clf.dequeue()                 #####\r\n","\r\n","              #passive_count = 0\r\n","              #best2,best1=outputs_clf.argsort()[-2:][::1]\r\n","            # top2_diff.append(float(outputs_clf[best1]-outputs_clf[best2]))\r\n","            # preds.append(outputs_clf)\r\n","            #  best1_lst.append(outputs_clf[best1])\r\n","            #  pred_cls.append(best1)\r\n","              #tmp=outputs_clf.argmax()\r\n","          #    print('\\n',k,'class:',best1,'\\tbest1:',outputs_clf[best1],'\\tdiff: ',outputs_clf[best1]-outputs_clf[best2])\r\n","              \r\n","              if self.clf_strategy == 'raw':\r\n","                  self.clf_selected_queue = outputs_clf\r\n","              elif self.clf_strategy == 'median':\r\n","                  self.clf_selected_queue = self.myqueue_clf.median\r\n","              elif self.clf_strategy == 'ma':\r\n","                  self.clf_selected_queue = self.myqueue_clf.ma\r\n","              elif self.clf_strategy == 'ewma':\r\n","                  self.clf_selected_queue = self.myqueue_clf.ewma\r\n","              \r\n","              \r\n","              \r\n","              best2,best1=self.clf_selected_queue.argsort()[-2:][::1]\r\n","              top2_diff=float(self.clf_selected_queue[best1]-self.clf_selected_queue[best2])\r\n","              \r\n","            #  print('best1:',best1,'\\ttop2_diff:',top2_diff)\r\n","              self.top2_diff_avg.append(top2_diff)\r\n","              self.pred_cls.append(best1)\r\n","            #  self.prev_best1=best1\r\n","              \r\n","            #   print('whole clafication part',time.time()-st)\r\n","              \r\n","              st=time.time()\r\n","              if top2_diff>self.top2_diff_threshold:\r\n","              #  print('prediction class: ',best1)\r\n","                self.pred_cls_queue.insert(0,best1)\r\n","                # self.prev_best1=best1\r\n","              else:\r\n","              #  print('prediction class: ',10)\r\n","                self.pred_cls_queue.insert(0,10)\r\n","              self.pred_cls_queue.pop()\r\n","              \r\n","\r\n","              self.post_processing(best1)\r\n","           \r\n","      elapsedTime = time.time() - t1\r\n","      fps = \"(Playback) {:.1f} FPS\".format(1/elapsedTime)\r\n","      # print(fps,'\\nspf',elapsedTime)                       ####\r\n","      # print()\r\n","    #  cv2.putText(frame, fps, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38, 0, 255), 1, cv2.LINE_AA)\r\n","      return self.final_prediction\r\n","       \r\n","  def post_processing(self,best1):\r\n","      count=1\r\n","      for i in range(self.pred_cls_queue_size-1):\r\n","        if self.pred_cls_queue[i]!=self.pred_cls_queue[i+1]:\r\n","            break\r\n","        count+=1\r\n","\r\n","\r\n","      if count>=self.freq_threshold:\r\n","        if self.pred_cls_queue[0]==6:\r\n","          if count>=self._6_freq_threshold:\r\n","            #print('prediction class: ',self.pred_cls_queue[0])\r\n","            self.final_prediction=self.pred_cls_queue[0]\r\n","            if self.pred_cls_queue[0] not in [10,self.prev_prediction]:\r\n","              self.play(self.pred_cls_queue[0])\r\n","              self.predictions_list.append(self.pred_cls_queue[0])\r\n","              self.prev_prediction=self.pred_cls_queue[0]\r\n","          else:\r\n","           # print('prediction class: ',10)\r\n","            self.final_prediction=10 \r\n","        else:\r\n","         # print('prediction class: ',self.pred_cls_queue[0])\r\n","          self.final_prediction=self.pred_cls_queue[0]\r\n","          if self.pred_cls_queue[0] not in [10,self.prev_prediction]:\r\n","              self.play(self.pred_cls_queue[0])\r\n","              self.predictions_list.append(self.pred_cls_queue[0])\r\n","              self.prev_prediction=self.pred_cls_queue[0]\r\n","\r\n","        if self.pred_cls_queue[0]==10 and count>=6:     \r\n","          self.prev_prediction=10\r\n","      else:\r\n","       # print('prediction class: ',10)\r\n","        self.final_prediction=10 \r\n","\r\n","  def accuracy_on_video(self,predictions_list,labels_list):\r\n","    no_actual_gestures=len(labels_list)\r\n","    return (1-LevenshteinDistance(predictions_list,labels_list)/no_actual_gestures)*100  \r\n","  \r\n","  def load_audio_files(self,path):\r\n","    import os\r\n","    str_to_num={'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9,'10':10}\r\n","    if not os.path.isdir(path):\r\n","       print('audio_files_directory does not exist')\r\n","       return\r\n","    files_lst=os.listdir(path)\r\n","\r\n","    self.key_to_audio={}\r\n","    for el in files_lst:\r\n","      ind=str_to_num[el[:-4]]\r\n","      self.key_to_audio[ind]=Audio(path+el,autoplay=True)\r\n","\r\n","  def play(self,gesture):\r\n","    display(self.key_to_audio[gesture])#,display_id='0')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nEBQgEznPALZ"},"source":["vid_path='/content/drive/My Drive/my_projects/ISLR'+'/ndata/wo_label/f1.mp4'\r\n","\r\n","rotate_code=check_rotation(vid_path)\r\n","pred=Prediction(rotate_code)                  ######\r\n","cap=cv2.VideoCapture(vid_path)\r\n","tf=cap.get(cv2.CAP_PROP_FRAME_COUNT)\r\n","k=0\r\n","try:\r\n","  while k<tf:\r\n","      #  print('\\n',k+1)\r\n","        _,frame=cap.read()\r\n","        outputs_clf=pred(frame)\r\n","        #print(outputs_clf)\r\n","        k+=1\r\n","finally:\r\n","  cap.release()\r\n","\r\n","pred.labels_list=[0,1,2,3,4,5,6,7,8,9]   # gestures in current video\r\n","\r\n","accuracy=pred.accuracy_on_video(pred.predictions_list,pred.labels_list)\r\n","print('True Gesture Class : ',pred.labels_list,'\\nPredicted Gesture Class : ',pred.predictions_list)\r\n","print('\\nAccuracy on '+vid_path.split('/')[-1]+' video : ' ,accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUDBQSuz4iaM"},"source":[""],"execution_count":null,"outputs":[]}]}