{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting data into train and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prcnt=15\n",
    "excel_in = pd.read_excel (r'E:\\\\Faiz\\\\Projects\\\\My_Projects\\\\ISLR\\my_data\\\\ntotal_data.xlsx')\n",
    "\n",
    "grouped = excel_in.groupby('creator',group_keys=False)\n",
    "valid_set=grouped.apply(lambda x: x.sample(n=int(len(x)*val_prcnt/100),random_state=1,replace=False))\n",
    "excel_in['subset']='training'\n",
    "excel_in.loc[list(valid_set.index),'subset']='validation'\n",
    "\n",
    "excel_in=excel_in.sample(frac=1,random_state=1)\n",
    "\n",
    "group=excel_in.groupby('subset',group_keys=False)\n",
    "excel_out=group.get_group('training')\n",
    "excel_out=excel_out.append(group.get_group('validation'))\n",
    "\n",
    "#excel_out.to_excel('E:\\\\Faiz\\\\Projects\\\\My_Projects\\\\ISLR\\\\my_data\\\\ntrain_val_test_set.xlsx',index=False)\n",
    "print(excel_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating annotation json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotation_file(df,ann_path,data_path='ndata',natural_flow=True):\n",
    "    nf=natural_flow\n",
    "    ann_dict={}\n",
    "    ann_dict[\"labels\"]=[\"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"none\"]\n",
    "    key_to_val={0: \"zero\",1: \"one\",2: \"two\",3: \"three\",4: \"four\",5: \"five\",6: \"six\",7: \"seven\",8: \"eight\",9: \"nine\",10:\"none\"}\n",
    "    dic=dict()\n",
    "\n",
    "    for i in range(len(df['video'])):\n",
    "        gest_path=data_path+'/'+df['video'][i]+'_'+str(df['gesture'][i])\n",
    "        label=key_to_val[tuple(df['label'])[i]]\n",
    "        sf=df['start frame'][i]\n",
    "        ef=df['end frame'][i]\n",
    "        tf=ef-sf+1\n",
    "        if nf and tf<16:\n",
    "            diff=16-tf\n",
    "            sf-=(diff+1)//2\n",
    "            ef+=diff//2\n",
    "        subset=df['subset'][i]\n",
    "        dic[gest_path]=dict(subset=subset,annotations=dict(label=label,start_frame=str(sf),end_frame=str(ef)))\n",
    "\n",
    "    ann_dict[\"database\"]=dic\n",
    "    print(ann_dict)\n",
    "\n",
    "    with open(ann_path,\"w\") as ann_file:\n",
    "        json.dump(ann_dict,ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_excel(r'E:\\\\Faiz\\\\Projects\\\\My_Projects\\\\ISLR\\\\my_data\\\\ntrain_set_w_none.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path='E:\\\\Faiz\\\\Projects\\\\My_Projects\\\\ISLR\\\\annotation\\\\annotation_0-9_w_nf_w_none_wo_val_test.json'\n",
    "make_annotation_file(df,ann_path,'ndata')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
